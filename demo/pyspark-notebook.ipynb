{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/spark-3.3.3-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-f3fddf4d-c4ed-48e0-817f-f2a8ac9ccc89;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.3.0 in central\n",
      "\tfound io.delta#delta-storage;2.3.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-core_2.12/2.3.0/delta-core_2.12-2.3.0.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-core_2.12;2.3.0!delta-core_2.12.jar (536ms)\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-storage/2.3.0/delta-storage-2.3.0.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-storage;2.3.0!delta-storage.jar (17ms)\n",
      "downloading https://repo1.maven.org/maven2/org/antlr/antlr4-runtime/4.8/antlr4-runtime-4.8.jar ...\n",
      "\t[SUCCESSFUL ] org.antlr#antlr4-runtime;4.8!antlr4-runtime.jar (48ms)\n",
      ":: resolution report :: resolve 1422ms :: artifacts dl 607ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.3.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.3.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   3   |   3   |   0   ||   3   |   3   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-f3fddf4d-c4ed-48e0-817f-f2a8ac9ccc89\n",
      "\tconfs: [default]\n",
      "\t3 artifacts copied, 0 already retrieved (4246kB/18ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/12/16 21:06:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "import delta as D\n",
    "from pathlib import Path\n",
    "\n",
    "# Create a SparkSession\n",
    "\n",
    "#initialize spark instance with delta extension\n",
    "builder = (SparkSession.builder.appName(\"pyspark-notebook\")\n",
    "            .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \n",
    "            .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "            )\n",
    "spark = D.configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "#staging folders\n",
    "landingStage = './data/00-landing'\n",
    "bronzeStage  = './data/01-bronze'\n",
    "silverStage  = './data/02-silver'\n",
    "goldStage    = './data/03-gold'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/00-landing/01-products.csv\n",
      "./data/00-landing/02-products.csv\n",
      "./data/00-landing/03-products.csv\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#create staging folder and dummy files (will overwrite file if exists)\n",
    "python3 ./init/create_dummy_files.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#landing to bronze\n",
    "\n",
    "#data schema\n",
    "data_schema = {\n",
    "        'ProductId':'integer'\n",
    "        ,'ProductNumber':'string'\n",
    "        ,'ProductName':'string'\n",
    "        ,'ModelName':'string'\n",
    "        ,'StandardCost':'integer'\n",
    "        ,'ListPrice':'integer'\n",
    "        ,'Timestamp':'timestamp'\n",
    "        }\n",
    "data_schema = [F.col(column_name).cast(data_type) for column_name,data_type in data_schema.items()]\n",
    "\n",
    "for csv_file in Path(landingStage).glob('*products.csv'):\n",
    "    df = spark.read.format(\"csv\").load(path=str(csv_file),header=True)\n",
    "\n",
    "    #casting and filtering\n",
    "    df = df.select(data_schema).dropDuplicates([\"ProductID\"])\n",
    "        \n",
    "    #export as parquet\n",
    "    bronze_file = Path(bronzeStage) / f'{Path(csv_file).stem}.parquet'\n",
    "    df.write.format(\"parquet\").mode('overwrite').save(str(bronze_file))\n",
    "    \n",
    "    print(f\"Filename: {bronze_file.name} nr. unique products: {df.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create empty unmanaged table in silver folder\n",
    "table_name = 'Products'\n",
    "silver_table = str((Path(silverStage) / 'products-delta').absolute())\n",
    "(D.DeltaTable.createIfNotExists(spark)\n",
    "    .tableName(table_name)\n",
    "    .addColumn('ProductID','INT')\n",
    "    .addColumn('ProductNumber','STRING')\n",
    "    .addColumn('ProductName','STRING')\n",
    "    .addColumn('ModelName','STRING')\n",
    "    .addColumn('StandardCost','INT')\n",
    "    .addColumn('ListPrice','INT')\n",
    "    .addColumn('Timestamp','TIMESTAMP')\n",
    "    .addColumn('ProfitMargin','DOUBLE')\n",
    "    .addColumn('ModelRank','INTEGER')\n",
    "    .location(silver_table)\n",
    "    .partitionedBy('ModelName')\n",
    "    .execute()\n",
    "    )\n",
    "\n",
    "#read delta table as instance\n",
    "dt = D.DeltaTable.forPath(spark, silver_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bronze to silver\n",
    "for bronze_file in Path(bronzeStage).glob('*.parquet'):\n",
    "    df = spark.read.format(\"parquet\").load(path=str(bronze_file))\n",
    "    \n",
    "    #calculate marging\n",
    "    margin = F.round(100-((F.col('StandardCost')/F.col('ListPrice'))*100),2)\n",
    "    df = df.withColumn('ProfitMargin',margin)\n",
    "    \n",
    "    #rank most profitable models \n",
    "    first_tier_models = (df.filter( (F.col('ProfitMargin') >50) & (F.col('StandardCost')>100) )\n",
    "                        .select('ModelName').distinct().toPandas().values.flatten().tolist())\n",
    "    second_tier_models = (df.filter( ( F.col('ProfitMargin').between(40,50) ) & (F.col('StandardCost')>100) )\n",
    "                        .select('ModelName').distinct().toPandas().values.flatten().tolist())\n",
    "    third_tier_models = (df.filter( ( F.col('ProfitMargin')<40 ) & (F.col('StandardCost')>100) )\n",
    "                        .select('ModelName').distinct().toPandas().values.flatten().tolist())\n",
    "    all_tier_models = (F.when(F.col('ModelName').isin(first_tier_models),1)\n",
    "                        .when(F.col('ModelName').isin(second_tier_models),2)\n",
    "                        .when(F.col('ModelName').isin(third_tier_models),3)\n",
    "                        .otherwise(4))\n",
    "\n",
    "    df = df.withColumn('ModelRank',all_tier_models)\n",
    "    \n",
    "    #upsert data to delta table\n",
    "    (dt.alias('target')\n",
    "        .merge(source=df.alias('source'),condition='target.ProductID = source.ProductID')\n",
    "        .whenMatchedUpdateAll()\n",
    "        .whenNotMatchedInsertAll()\n",
    "        .execute())\n",
    "    \n",
    "print(f\"Tablename: {silver_table} total nr. unique products: {dt.toDF().count()}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#silver to golden\n",
    "dt = spark.read.format(\"delta\").load(silver_table)\n",
    "dt_sale = dt.select('ProductID','StandardCost','ListPrice','ProfitMargin')\n",
    "dt_products = dt.select('ProductID','ProductNumber','ProductName','ModelName','ModelRank')\n",
    "\n",
    "golden_sale_table = str((Path(goldStage) / 'sale-delta').absolute())\n",
    "golden_products_table = str((Path(goldStage) / 'products-delta').absolute())\n",
    "dt_sale.write.format('delta').mode('overwrite').option('path',golden_sale_table).saveAsTable('Sales_gold')\n",
    "dt_products.write.format('delta').mode('overwrite').option('path',golden_products_table).saveAsTable('Products_gold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
